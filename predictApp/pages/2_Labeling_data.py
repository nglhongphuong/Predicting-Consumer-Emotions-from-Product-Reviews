import streamlit as st
import pandas as pd
import joblib
import re
from underthesea import word_tokenize, text_normalize

# ===== Thiáº¿t láº­p giao diá»‡n =====
st.set_page_config(page_title="Dá»± ÄoÃ¡n Cáº£m XÃºc Theo File", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ Dá»± ÄoÃ¡n Cáº£m XÃºc Tá»« File BÃ¬nh Luáº­n KhÃ¡ch HÃ ng")

st.markdown("""
#### ğŸ‘‰ Táº£i lÃªn file `.csv` chá»©a cá»™t `content` (bÃ¬nh luáº­n).
Sau Ä‘Ã³, há»‡ thá»‘ng sáº½:
- LÃ m sáº¡ch dá»¯ liá»‡u
- Dá»± Ä‘oÃ¡n cáº£m xÃºc theo mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n (Page 1)
- Tráº£ vá» file `.csv` Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n nhÃ£n Ä‘á»ƒ báº¡n táº£i vá»
""")

# ===== Load model vÃ  vectorizer Ä‘Ã£ huáº¥n luyá»‡n á»Ÿ Page 1 =====
model = joblib.load("sentiment_model.pkl")
vectorizer = joblib.load("count_vectorizer.pkl")

# ===== Bá»™ lá»c & xá»­ lÃ½ vÄƒn báº£n =====
emoji_pattern = re.compile(
    "[" + "".join([
        u"\U0001F600-\U0001F64F", u"\U0001F300-\U0001F5FF", u"\U0001F680-\U0001F6FF",
        u"\U0001F1E0-\U0001F1FF", u"\U00002702-\U000027B0", u"\U000024C2-\U0001F251",
        u"\U0001F926-\U0001F937", u"\U00010000-\U0010FFFF", u"\u200d", u"\u2640-\u2642",
        u"\u2600-\u2B55", u"\u23cf", u"\u23e9", u"\u231a", u"\u3030", u"\ufe0f"
    ]) + "]+", flags=re.UNICODE
)

short_word_dict = {
    "ko": "khÃ´ng", "kg": "khÃ´ng", "khong": "khÃ´ng", "k": "khÃ´ng", "kh": "khÃ´ng",
    "cx": "cÅ©ng", "mik": "mÃ¬nh", "mn": "má»i ngÆ°á»i", "bt": "bÃ¬nh thÆ°á»ng",
    "nv": "nhÃ¢n viÃªn", "sp": "sáº£n pháº©m", "Ä‘c": "Ä‘Æ°á»£c", "dc": "Ä‘Æ°á»£c",
    "Ä‘k": "Ä‘iá»u khoáº£n", "Ä‘t": "Ä‘iá»‡n thoáº¡i", "j": "gÃ¬", "vs": "vá»›i",
    "hok": "khÃ´ng", "lun": "luÃ´n", "z": "gÃ¬", "záº­y": "gÃ¬ váº­y", "thik": "thÃ­ch",
    "hum": "hÃ´m", "wa": "qua", "m": "mÃ¬nh", "mk": "mÃ¬nh", "bn": "báº¡n", "ok": "á»•n"
}

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    text = emoji_pattern.sub(r'', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = text_normalize(text)
    words = text.split()
    words = [short_word_dict.get(word, word) for word in words]
    text = ' '.join(words)
    text = word_tokenize(text, format="text")
    return text

# ===== Giao diá»‡n upload file =====
uploaded_file = st.file_uploader("ğŸ“¤ Táº£i lÃªn file CSV chá»©a cá»™t `content`", type=["csv"])

if uploaded_file is not None:
    try:
        df = pd.read_csv(uploaded_file)
        if 'content' not in df.columns:
            st.error("âŒ File cáº§n cÃ³ cá»™t tÃªn lÃ  `content`.")
        else:
            with st.spinner("ğŸ”„ Äang xá»­ lÃ½ vÃ  dá»± Ä‘oÃ¡n..."):
                df['clean_content'] = df['content'].apply(clean_text)
                features = vectorizer.transform(df['clean_content'])
                df['sentiment'] = model.predict(features)

                # Cho táº£i xuá»‘ng
                st.success("âœ… Xá»­ lÃ½ thÃ nh cÃ´ng! Nháº¥n nÃºt dÆ°á»›i Ä‘á»ƒ táº£i file káº¿t quáº£.")
                st.download_button(
                    label="ğŸ“¥ Táº£i file káº¿t quáº£",
                    data=df.to_csv(index=False).encode('utf-8-sig'),
                    file_name="ket_qua_du_doan.csv",
                    mime="text/csv"
                )

                st.dataframe(df[['content', 'sentiment']])
    except Exception as e:
        st.error(f"âŒ Lá»—i khi xá»­ lÃ½ file: {e}")
