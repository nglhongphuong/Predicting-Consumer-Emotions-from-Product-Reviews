import streamlit as st
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import joblib
from underthesea import word_tokenize # B·ªô ng·ªØ nghƒ©a
from underthesea import text_normalize #Chu·∫©n h√≥a vƒÉn b·∫£n
import re

# Giao di·ªán Streamlit
st.set_page_config(page_title="D·ª± ƒêo√°n C·∫£m X√∫c Kh√°ch H√†ng", page_icon="üòä", layout="wide")
st.title("D·ª± ƒêo√°n C·∫£m X√∫c Kh√°ch H√†ng T·ª´ B√¨nh Lu·∫≠n S·∫£n Ph·∫©m")

# Ph·∫ßn h∆∞·ªõng d·∫´n
st.markdown("""
    ### Nh·∫≠p b√¨nh lu·∫≠n c·ªßa b·∫°n d∆∞·ªõi ƒë√¢y v√† ch√∫ng t√¥i s·∫Ω d·ª± ƒëo√°n c·∫£m x√∫c c·ªßa kh√°ch h√†ng.
    **M√¥ h√¨nh c·ªßa ch√∫ng t√¥i ph√¢n lo·∫°i th√†nh 2 nh√£n:** 
    - **H√†i l√≤ng**
    - **Kh√¥ng h√†i l√≤ng**
""")


#Danh s√°ch emoji c·∫ßn x√≥a
emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"  # M·∫∑t c∆∞·ªùi, c·∫£m x√∫c
        u"\U0001F300-\U0001F5FF"  # Bi·ªÉu t∆∞·ª£ng thi√™n nhi√™n, ƒë·ªëi t∆∞·ª£ng
        u"\U0001F680-\U0001F6FF"  # Bi·ªÉu t∆∞·ª£ng giao th√¥ng
        u"\U0001F1E0-\U0001F1FF"  # C·ªù qu·ªëc gia
        u"\U00002702-\U000027B0"  # K√Ω hi·ªáu d·∫°ng h√¨nh
        u"\U000024C2-\U0001F251"  # K√Ω hi·ªáu ƒë√≥ng khung
        u"\U0001F926-\U0001F937"  # H√†nh ƒë·ªông, c·ª≠ ch·ªâ
        u"\U00010000-\U0010FFFF" # C√°c k√Ω t·ª± b·ªï sung
        u"\u200d"                 # N·ªëi k√Ω t·ª± (zero width joiner)
        u"\u2640-\u2642"          # K√Ω hi·ªáu gi·ªõi t√≠nh
        u"\u2600-\u2B55"          # Bi·ªÉu t∆∞·ª£ng ƒëa d·∫°ng kh√°c
        u"\u23cf"                 # Bi·ªÉu t∆∞·ª£ng ƒë·∫©y ƒëƒ©a
        u"\u23e9"                 # Tua nhanh
        u"\u231a"                 # ƒê·ªìng h·ªì
        u"\u3030"                 # D·∫•u ngo·∫±n ngo√®o
        u"\ufe0f"                 # B·ªô ch·ªçn ki·ªÉu hi·ªÉn th·ªã
        "]+", flags=re.UNICODE
)
#Load danh s√°ch c√°c t·ª´ vi·∫øt t·∫Øt hay s·ª≠ d·ª•ng
short_word_dict = {
    "ko": "kh√¥ng",
    "kg": "kh√¥ng",
    "khong": "kh√¥ng",
    "k": "kh√¥ng",
    "kh": "kh√¥ng",
    "cx": "c≈©ng",
    "mik": "m√¨nh",
    "mn": "m·ªçi ng∆∞·ªùi",
    "bt": "b√¨nh th∆∞·ªùng",
    "nv": "nh√¢n vi√™n",
    "sp": "s·∫£n ph·∫©m",
    "ƒëc": "ƒë∆∞·ª£c",
    "dc": "ƒë∆∞·ª£c",
    "ƒëk": "ƒëi·ªÅu kho·∫£n",
    "ƒët": "ƒëi·ªán tho·∫°i",
    "j": "g√¨",
    "vs": "v·ªõi",
    "hok": "kh√¥ng",
    "lun": "lu√¥n",
    "z": "g√¨",
    "z·∫≠y": "g√¨ v·∫≠y",
    "thik": "th√≠ch",
    "hum": "h√¥m",
    "wa": "qua",
    "m": "m√¨nh",
    "mk": "m√¨nh",
    "bn": "b·∫°n",
    "ok": "·ªïn"
}

def clean_data(text):
    text = text.lower()
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    text = emoji_pattern.sub(r'', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = text_normalize(text)
    words = text.split()
    words = [short_word_dict.get(word, word) for word in words]
    text = ' '.join(words)
    text = word_tokenize(text, format="text")
    return text

# H√†m hu·∫•n luy·ªán m√¥ h√¨nh
def train_model(df):
    # Vectorizer v√† m√¥ h√¨nh
    vectorizer = CountVectorizer()
    model = LogisticRegression()

    # Chia d·ªØ li·ªáu train/test
    train_sentences, test_sentences, train_labels, test_labels = train_test_split(
        df['clean_content'],
        df['title'],
        test_size=0.1,
        random_state=42
    )

    # Vector h√≥a
    train_features = vectorizer.fit_transform(train_sentences)
    model.fit(train_features, train_labels)

    return model, vectorizer

# S·ª≠ d·ª•ng st.cache_resource cho vi·ªác l∆∞u m√¥ h√¨nh v√† vectorizer
@st.cache_resource
def load_data():
    sheet_id = "1GTo2SGUjDUA4T1mBaUwIkiOhCmfcRaIygP-5KTb5Gl0"
    csv_url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv"
    df = pd.read_csv(csv_url)
    df = df.fillna('')  # Thay th·∫ø NaN b·∫±ng chu·ªói r·ªóng trong to√†n b·ªô DataFrame
    return df
# T·∫£i d·ªØ li·ªáu
df_balanced_2_label = load_data()

# N·∫øu m√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán v√† l∆∞u, ta s·∫Ω hu·∫•n luy·ªán v√† l∆∞u m√¥ h√¨nh
model_file = 'sentiment_model.pkl'
vectorizer_file = 'count_vectorizer.pkl'

# N·∫øu m√¥ h√¨nh ƒë√£ t·ªìn t·∫°i, t·∫£i m√¥ h√¨nh v√† vectorizer ƒë√£ l∆∞u
try:
    model = joblib.load(model_file)
    vectorizer = joblib.load(vectorizer_file)
except FileNotFoundError:
    # N·∫øu ch∆∞a c√≥ m√¥ h√¨nh, hu·∫•n luy·ªán v√† l∆∞u l·∫°i
    model, vectorizer = train_model(df_balanced_2_label)
    joblib.dump(model, model_file)
    joblib.dump(vectorizer, vectorizer_file)

# Nh·∫≠p d·ªØ li·ªáu vƒÉn b·∫£n t·ª´ ng∆∞·ªùi d√πng
user_input = st.text_area("Nh·∫≠p b√¨nh lu·∫≠n c·ªßa kh√°ch h√†ng:")

if st.button("D·ª± ƒëo√°n c·∫£m x√∫c"):
    if user_input:
        user_input = clean_data(user_input)
        # Vector h√≥a vƒÉn b·∫£n ng∆∞·ªùi d√πng nh·∫≠p
        user_vector = vectorizer.transform([user_input])
        # D·ª± ƒëo√°n c·∫£m x√∫c
        prediction = model.predict(user_vector)

        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        if prediction[0] == "c·ª±c k·ª≥ h√†i l√≤ng":
            st.success("üëç Kh√°ch h√†ng **h√†i l√≤ng** v·ªõi s·∫£n ph·∫©m.")
        else:
            st.error("üëé Kh√°ch h√†ng **kh√¥ng h√†i l√≤ng** v·ªõi s·∫£n ph·∫©m.")
    else:
        st.warning("Vui l√≤ng nh·∫≠p b√¨nh lu·∫≠n ƒë·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c.")
